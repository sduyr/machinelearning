{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/davidic2ofu/develop/project/negative-words.txt', 'r+', encoding = \"ISO-8859-1\") as n:\n",
    "    negative_words = [p.strip('\\n') for p in n.readlines()]\n",
    "with open('/Users/davidic2ofu/develop/project/positive-words.txt', 'r+', encoding = \"ISO-8859-1\") as n:\n",
    "    positive_words = [p.strip('\\n') for p in n.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/davidic2ofu/develop/project/training.txt', 'r+') as n:\n",
    "    raw_training_data = [line.strip('\\n').split('\\t') for line in n.readlines()]\n",
    "train_data = [d[0] for d in raw_training_data]\n",
    "train_target = [d[1] for d in raw_training_data]\n",
    "with open('/Users/davidic2ofu/develop/project/test.txt', 'r+') as n:\n",
    "    raw_test_data = [line.strip('\\n').split('\\t') for line in n.readlines()]\n",
    "test_data = [d[0] for d in raw_test_data]\n",
    "test_target = [d[1] for d in raw_test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tvec = TfidfVectorizer()\n",
    "train_tfidf = tvec.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (i) 5-fold cross validation on training.txt\n",
    "#\n",
    "# To determine optimal configurations for each classifier, I collected mean accuracy scores from\n",
    "# running 5-fold cross validation.  Once I arrived at an optimal setting on this basis, I got\n",
    "# the precision, recall, and f1 scores at that setting.\n",
    "# I think this is reasonable considering there is an even distribution of ratings across the datasets--\n",
    "# mean that of 10000 total examples in the dataset, 2000 have a rating of 1, 2000 have a rating of 2, and so on.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultinomialNB\n",
    "#\n",
    "# No parameters were specified to change from project instructions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_PRECISION_MACRO  : 0.42504131194283434\n",
      "TEST_RECALL_MACRO     : 0.3913\n",
      "TEST_F1_MACRO         : 0.389894922172691\n"
     ]
    }
   ],
   "source": [
    "results = cross_validate(nb, train_tfidf, train_target, cv=5, return_train_score=False, scoring=('precision_macro', 'recall_macro', 'f1_macro'))\n",
    "for key in ['test_precision_macro', 'test_recall_macro', 'test_f1_macro']:\n",
    "    print('{0: <22}: {1}'.format(key.upper(), results[key].mean()))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "#\n",
    "# tried L1 and L2 regularization with C parameters 0.001, 0.01, 0.1, 1, 10, 100...\n",
    "# L2 regularization gave highest accuracy with C parameter value of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2, 0.2, 0.3937, 0.46399999999999997, 0.42610000000000003, 0.4091, 0.4192, 0.42469999999999997, 0.44880000000000003, 0.4664, 0.44719999999999993, 0.42219999999999996]\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for p in ['l1', 'l2']:\n",
    "    for c in [0.001,0.01,0.1,1,10,100]:\n",
    "        lr = LogisticRegression(solver='liblinear', multi_class='ovr', penalty=p, C=c)\n",
    "        scores.append(cross_val_score(lr, train_tfidf, train_target, cv=5).mean())\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORING REPORT\n",
      "==============\n",
      "AVERAGE PRECISION: 0.45945328665208207\n",
      "AVERAGE RECALL: 0.46640000000000004\n",
      "AVERAGE F1: 0.4569426339889883\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='liblinear', multi_class='ovr', penalty='l2', C=1)\n",
    "precision = cross_val_score(lr, train_tfidf, train_target, cv=5, scoring='precision_macro').mean()\n",
    "recall = cross_val_score(lr, train_tfidf, train_target, cv=5, scoring='recall_macro').mean()\n",
    "f1 = cross_val_score(lr, train_tfidf, train_target, cv=5, scoring='f1_macro').mean()\n",
    "print('SCORING REPORT\\n{}\\nAVERAGE PRECISION: {}\\nAVERAGE RECALL: {}\\nAVERAGE F1: {}'.format('='*14, precision, recall ,f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network: MLPClassifier\n",
    "#\n",
    "# tried 5, 10, 15, 20, and 100 units on 1, 2, 3 hidden layers\n",
    "#\n",
    "# 1 hidden layer with 100 units gave the highest accuracy; however... \n",
    "# some testing shows below that this setting results in folds where some labels are never predicted,\n",
    "# resulting in undefined precision, recall, and f1 scores.  The returned score for those labels is 0,\n",
    "# bringing the average scores way down.\n",
    "#\n",
    "# Therefore, I chose a different setting:\n",
    "# 3 hidden layers with 5 units per layer gave a high accuracy and did not result in undefined scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "all_info = ''\n",
    "for num_hidden_layers in [1, 2, 3]:\n",
    "    nn_test = MLPClassifier(hidden_layer_sizes=(100, num_hidden_layers))\n",
    "    start = time()\n",
    "    score = cross_val_score(nn_test, train_tfidf, train_target, cv=5).mean()\n",
    "    info = '{} hidden layers, {} units per layer\\nAVERAGE SCORE: {} ({} seconds)\\n\\n'.format(num_hidden_layers, 100, score, time() - start)\n",
    "    print(info)\n",
    "    all_info += info\n",
    "print(all_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Results:\n",
    "\n",
    "1 hidden layers, 5 units per layer\n",
    "AVERAGE SCORE: 0.26189999999999997 (82.26306986808777 seconds)\n",
    "\n",
    "1 hidden layers, 10 units per layer\n",
    "AVERAGE SCORE: 0.2612 (171.13859677314758 seconds)\n",
    "\n",
    "1 hidden layers, 15 units per layer\n",
    "AVERAGE SCORE: 0.26639999999999997 (225.12408924102783 seconds)\n",
    "\n",
    "1 hidden layers, 20 units per layer\n",
    "AVERAGE SCORE: 0.3154 (379.20189094543457 seconds)\n",
    "\n",
    "2 hidden layers, 5 units per layer\n",
    "AVERAGE SCORE: 0.3307 (164.73012685775757 seconds)\n",
    "\n",
    "2 hidden layers, 10 units per layer\n",
    "AVERAGE SCORE: 0.3281 (220.51628804206848 seconds)\n",
    "\n",
    "2 hidden layers, 15 units per layer\n",
    "AVERAGE SCORE: 0.279 (226.49298191070557 seconds)\n",
    "\n",
    "2 hidden layers, 20 units per layer\n",
    "AVERAGE SCORE: 0.25170000000000003 (174.7208309173584 seconds)\n",
    "\n",
    "3 hidden layers, 5 units per layer\n",
    "AVERAGE SCORE: 0.3698 (136.1033742427826 seconds)\n",
    "\n",
    "3 hidden layers, 10 units per layer\n",
    "AVERAGE SCORE: 0.3613 (231.52910900115967 seconds)\n",
    "\n",
    "3 hidden layers, 15 units per layer\n",
    "AVERAGE SCORE: 0.30289999999999995 (345.6000349521637 seconds)\n",
    "\n",
    "3 hidden layers, 20 units per layer\n",
    "AVERAGE SCORE: 0.3466 (331.38618206977844 seconds)\n",
    "\n",
    "1 hidden layers, 100 units per layer\n",
    "AVERAGE SCORE: 0.4095000000000001\n",
    "\n",
    "2 hidden layers, 100 units per layer\n",
    "AVERAGE SCORE: 0.2345 (702.7552897930145 seconds)\n",
    "\n",
    "3 hidden layers, 100 units per layer\n",
    "AVERAGE SCORE: 0.3698 (1494.6928179264069 seconds)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(100, 1)) # showing 1 hidden layer, 100 units, resulting in low score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.19209275852557992"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(clf, train_tfidf, train_target, cv=5, scoring='precision_macro').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3965408692188382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(5, 3)) # compare with 3 hidden layers, 5 units\n",
    "print(cross_val_score(clf, train_tfidf, train_target, cv=5, scoring='precision_macro').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_PRECISION_MACRO  : 0.3863551431751554\n",
      "TEST_RECALL_MACRO     : 0.3752\n",
      "TEST_F1_MACRO         : 0.37198399981821356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "results = cross_validate(clf, train_tfidf, train_target, cv=5, return_train_score=False, scoring=('precision_macro', 'recall_macro', 'f1_macro'))\n",
    "for key in ['test_precision_macro', 'test_recall_macro', 'test_f1_macro']:\n",
    "    print('{0: <22}: {1}'.format(key.upper(), results[key].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoostClassifier\n",
    "#\n",
    "# tried n_estimator values of 25, 50, 75...\n",
    "# default n_estimator value of 50 gave the best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.40199999999999997, 0.4272, 0.4232]\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for n in [25, 50, 75]:\n",
    "    ada = AdaBoostClassifier(n_estimators=n)\n",
    "    scores.append(cross_val_score(ada, train_tfidf, train_target, cv=5).mean())\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORING REPORT\n",
      "==============\n",
      "AVERAGE PRECISION: 0.4245235816378587\n",
      "AVERAGE RECALL: 0.4272\n",
      "AVERAGE F1: 0.42239110439415617\n"
     ]
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(n_estimators=50)\n",
    "precision = cross_val_score(clf, train_tfidf, train_target, cv=5, scoring='precision_macro').mean()\n",
    "recall = cross_val_score(clf, train_tfidf, train_target, cv=5, scoring='recall_macro').mean()\n",
    "f1 = cross_val_score(clf, train_tfidf, train_target, cv=5, scoring='f1_macro').mean()\n",
    "print('SCORING REPORT\\n{}\\nAVERAGE PRECISION: {}\\nAVERAGE RECALL: {}\\nAVERAGE F1: {}'.format('='*14, precision, recall ,f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine SVC\n",
    "#\n",
    "# tried linear, polynomial, rbf, and sigmoid kernels with cost factors 1, 10, 100, 1000...\n",
    "# cost factor 1000 with rbf kernel gave highest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [1, 10, 100, 1000]:\n",
    "    for k in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "        svm = SVC(gamma='auto', C=c, kernel=k)\n",
    "        result = cross_val_score(svm, train_tfidf, train_target, cv=5).mean()\n",
    "        print('cost factor {}, kernel {}: {}'.format(c, k , result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Results:\n",
    "\n",
    "cost factor 1, kernel linear: 0.4702\n",
    "cost factor 1, kernel poly: 0.2688\n",
    "cost factor 1, kernel rbf: 0.36129999999999995\n",
    "cost factor 1, kernel sigmoid: 0.36150000000000004\n",
    "cost factor 10, kernel linear: 0.4372\n",
    "cost factor 10, kernel poly: 0.2699\n",
    "cost factor 10, kernel rbf: 0.36129999999999995\n",
    "cost factor 10, kernel sigmoid: 0.36150000000000004\n",
    "cost factor 100, kernel linear: 0.42969999999999997\n",
    "cost factor 100, kernel poly: 0.2697\n",
    "cost factor 100, kernel rbf: 0.36129999999999995\n",
    "cost factor 100, kernel sigmoid: 0.36150000000000004\n",
    "cost factor 1000, kernel linear: 0.43100000000000005\n",
    "cost factor 1000, kernel poly: 0.2697\n",
    "cost factor 1000, kernel rbf: 0.4425\n",
    "cost factor 1000, kernel sigmoid: 0.41200000000000003\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_PRECISION_MACRO  : 0.4458132278366751\n",
      "TEST_RECALL_MACRO     : 0.4425\n",
      "TEST_F1_MACRO         : 0.4345959450940112\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(gamma='auto', C=1000, kernel='rbf')\n",
    "results = cross_validate(clf, train_tfidf, train_target, cv=5, return_train_score=False, scoring=('precision_macro', 'recall_macro', 'f1_macro'))\n",
    "for key in ['test_precision_macro', 'test_recall_macro', 'test_f1_macro']:\n",
    "    print('{0: <22}: {1}'.format(key.upper(), results[key].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (ii) 5-fold cross validation with additional knowledge into model (filter train data by sentiment words)\n",
    "#\n",
    "# Also tested accuracy of these classifiers on filtered test data - see bottom of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_words = negative_words + positive_words\n",
    "filtered_train_data = []\n",
    "for doc in train_data:\n",
    "    filtered_word_list = []\n",
    "    word_list = doc.split()\n",
    "    for word in word_list:\n",
    "        word = word.strip(',.-;()[]').lower()\n",
    "        if word in sentiment_words:\n",
    "            filtered_word_list.append(word)\n",
    "    filtered_doc = ' '.join(filtered_word_list)\n",
    "    filtered_train_data.append(filtered_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_train_tfidf = tvec.fit_transform(filtered_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARISON TO UNFILTERED TRAINING DATA:\n",
    "#\n",
    "# MultinomialNB: filtered is higher\n",
    "# AdaBoostClassifier: filtered is lower\n",
    "# MLPClassifier: filtered is lower (recall is about the same)\n",
    "# LogisticRegression: filtered is lower\n",
    "# SVC: about the same\n",
    "#\n",
    "# Unfiltered training data generally resulted in better accuracy in cross validation.\n",
    "# This probably indicates that better context can be drawn from the full reviews than from the reviews\n",
    "# filtered by the sentiment words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "RESULTS FOR MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "TEST_PRECISION_MACRO  : 0.4537654758377938\n",
      "TEST_RECALL_MACRO     : 0.45170000000000005\n",
      "TEST_F1_MACRO         : 0.4496737745893431\n",
      "\n",
      "\n",
      "RESULTS FOR AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "TEST_PRECISION_MACRO  : 0.40878374635803566\n",
      "TEST_RECALL_MACRO     : 0.4199\n",
      "TEST_F1_MACRO         : 0.4066466723679296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "RESULTS FOR MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(5, 3), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "TEST_PRECISION_MACRO  : 0.36235554347162413\n",
      "TEST_RECALL_MACRO     : 0.37120000000000003\n",
      "TEST_F1_MACRO         : 0.358345443443277\n",
      "\n",
      "\n",
      "RESULTS FOR LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "TEST_PRECISION_MACRO  : 0.4358384669421536\n",
      "TEST_RECALL_MACRO     : 0.4471\n",
      "TEST_F1_MACRO         : 0.43558902768041896\n",
      "\n",
      "\n",
      "RESULTS FOR SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "TEST_PRECISION_MACRO  : 0.44470675057931947\n",
      "TEST_RECALL_MACRO     : 0.4479\n",
      "TEST_F1_MACRO         : 0.4395850713449826\n"
     ]
    }
   ],
   "source": [
    "for clf in [MultinomialNB(),\n",
    "            AdaBoostClassifier(n_estimators=50),\n",
    "            MLPClassifier(hidden_layer_sizes=(5, 3)),\n",
    "            LogisticRegression(solver='liblinear', multi_class='ovr', penalty='l2', C=1),\n",
    "            SVC(gamma='auto', C=1000, kernel='rbf')\n",
    "           ]:\n",
    "    results = cross_validate(clf, filtered_train_tfidf, train_target, cv=5, return_train_score=False, scoring=('precision_macro', 'recall_macro', 'f1_macro'))\n",
    "    print('\\n\\nRESULTS FOR {}'.format(clf))\n",
    "    for key in ['test_precision_macro', 'test_recall_macro', 'test_f1_macro']:\n",
    "        print('{0: <22}: {1}'.format(key.upper(), results[key].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (iii) evaluation on test dataset\n",
    "#\n",
    "# In every case, reviews with ratings 2-4 were generally hard to predict.  Specifically, reviews with \n",
    "# a rating of 3 were the hardest to predict.  Reviews with ratings of 1 and 5 were predicted most accuractely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "test_tfidf = tvec.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "RESULTS FOR MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.51      0.67      0.57       200\n",
      "         2.0       0.51      0.39      0.44       200\n",
      "         3.0       0.42      0.44      0.43       200\n",
      "         4.0       0.45      0.48      0.47       200\n",
      "         5.0       0.72      0.57      0.64       200\n",
      "\n",
      "   micro avg       0.51      0.51      0.51      1000\n",
      "   macro avg       0.52      0.51      0.51      1000\n",
      "weighted avg       0.52      0.51      0.51      1000\n",
      "\n",
      "\n",
      "\n",
      "RESULTS FOR AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.58      0.59      0.59       200\n",
      "         2.0       0.40      0.34      0.37       200\n",
      "         3.0       0.39      0.37      0.38       200\n",
      "         4.0       0.40      0.37      0.38       200\n",
      "         5.0       0.53      0.67      0.59       200\n",
      "\n",
      "   micro avg       0.47      0.47      0.47      1000\n",
      "   macro avg       0.46      0.47      0.46      1000\n",
      "weighted avg       0.46      0.47      0.46      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "RESULTS FOR MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(5, 3), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.48      0.55      0.51       200\n",
      "         2.0       0.39      0.45      0.42       200\n",
      "         3.0       0.40      0.34      0.37       200\n",
      "         4.0       0.41      0.45      0.43       200\n",
      "         5.0       0.58      0.45      0.51       200\n",
      "\n",
      "   micro avg       0.45      0.45      0.45      1000\n",
      "   macro avg       0.45      0.45      0.45      1000\n",
      "weighted avg       0.45      0.45      0.45      1000\n",
      "\n",
      "\n",
      "\n",
      "RESULTS FOR LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.54      0.80      0.64       200\n",
      "         2.0       0.52      0.34      0.41       200\n",
      "         3.0       0.45      0.39      0.42       200\n",
      "         4.0       0.47      0.47      0.47       200\n",
      "         5.0       0.66      0.68      0.67       200\n",
      "\n",
      "   micro avg       0.53      0.53      0.53      1000\n",
      "   macro avg       0.53      0.53      0.52      1000\n",
      "weighted avg       0.53      0.53      0.52      1000\n",
      "\n",
      "\n",
      "\n",
      "RESULTS FOR SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.55      0.69      0.61       200\n",
      "         2.0       0.49      0.46      0.47       200\n",
      "         3.0       0.44      0.42      0.43       200\n",
      "         4.0       0.48      0.39      0.43       200\n",
      "         5.0       0.63      0.68      0.65       200\n",
      "\n",
      "   micro avg       0.53      0.53      0.53      1000\n",
      "   macro avg       0.52      0.53      0.52      1000\n",
      "weighted avg       0.52      0.53      0.52      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for clf in [MultinomialNB(),\n",
    "            AdaBoostClassifier(n_estimators=50),\n",
    "            MLPClassifier(hidden_layer_sizes=(5, 3)),\n",
    "            LogisticRegression(solver='liblinear', multi_class='ovr', penalty='l2', C=1),\n",
    "            SVC(gamma='auto', C=1000, kernel='rbf')\n",
    "           ]:\n",
    "    clf_fit = clf.fit(train_tfidf, train_target)\n",
    "    predicted = clf_fit.predict(test_tfidf)\n",
    "    print('\\n\\nRESULTS FOR {}'.format(clf))\n",
    "    print(classification_report(test_target, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FYI -- Predictions made on FILTERED test data with classifiers trained on the filtered training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_test_data = []\n",
    "for doc in test_data:\n",
    "    filtered_word_list = []\n",
    "    word_list = doc.split()\n",
    "    for word in word_list:\n",
    "        word = word.strip(',.-;()[]').lower()\n",
    "        if word in sentiment_words:\n",
    "            filtered_word_list.append(word)\n",
    "    filtered_doc = ' '.join(filtered_word_list)\n",
    "    filtered_test_data.append(filtered_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_train_tfidf = tvec.transform(filtered_train_data)\n",
    "filtered_test_tfidf = tvec.transform(filtered_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "RESULTS FOR MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.63      0.54      0.58       200\n",
      "         2.0       0.43      0.38      0.41       200\n",
      "         3.0       0.34      0.42      0.38       200\n",
      "         4.0       0.36      0.40      0.38       200\n",
      "         5.0       0.54      0.51      0.52       200\n",
      "\n",
      "   micro avg       0.45      0.45      0.45      1000\n",
      "   macro avg       0.46      0.45      0.45      1000\n",
      "weighted avg       0.46      0.45      0.45      1000\n",
      "\n",
      "\n",
      "\n",
      "RESULTS FOR AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.50      0.60      0.55       200\n",
      "         2.0       0.44      0.29      0.35       200\n",
      "         3.0       0.31      0.30      0.31       200\n",
      "         4.0       0.35      0.28      0.31       200\n",
      "         5.0       0.46      0.62      0.53       200\n",
      "\n",
      "   micro avg       0.42      0.42      0.42      1000\n",
      "   macro avg       0.41      0.42      0.41      1000\n",
      "weighted avg       0.41      0.42      0.41      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "RESULTS FOR MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(5, 3), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.50      0.54      0.52       200\n",
      "         2.0       0.43      0.40      0.42       200\n",
      "         3.0       0.38      0.28      0.32       200\n",
      "         4.0       0.36      0.49      0.42       200\n",
      "         5.0       0.47      0.41      0.44       200\n",
      "\n",
      "   micro avg       0.42      0.42      0.42      1000\n",
      "   macro avg       0.43      0.42      0.42      1000\n",
      "weighted avg       0.43      0.42      0.42      1000\n",
      "\n",
      "\n",
      "\n",
      "RESULTS FOR LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.58      0.62      0.60       200\n",
      "         2.0       0.43      0.34      0.38       200\n",
      "         3.0       0.37      0.41      0.39       200\n",
      "         4.0       0.38      0.36      0.37       200\n",
      "         5.0       0.53      0.56      0.54       200\n",
      "\n",
      "   micro avg       0.46      0.46      0.46      1000\n",
      "   macro avg       0.46      0.46      0.46      1000\n",
      "weighted avg       0.46      0.46      0.46      1000\n",
      "\n",
      "\n",
      "\n",
      "RESULTS FOR SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.55      0.60      0.57       200\n",
      "         2.0       0.43      0.44      0.43       200\n",
      "         3.0       0.38      0.41      0.39       200\n",
      "         4.0       0.40      0.31      0.35       200\n",
      "         5.0       0.58      0.59      0.59       200\n",
      "\n",
      "   micro avg       0.47      0.47      0.47      1000\n",
      "   macro avg       0.47      0.47      0.47      1000\n",
      "weighted avg       0.47      0.47      0.47      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for clf in [MultinomialNB(),\n",
    "            AdaBoostClassifier(n_estimators=50),\n",
    "            MLPClassifier(hidden_layer_sizes=(5, 3)),\n",
    "            LogisticRegression(solver='liblinear', multi_class='ovr', penalty='l2', C=1),\n",
    "            SVC(gamma='auto', C=1000, kernel='rbf')\n",
    "           ]:\n",
    "    clf_fit = clf.fit(filtered_train_tfidf, train_target)\n",
    "    predicted = clf_fit.predict(filtered_test_tfidf)\n",
    "    print('\\n\\nRESULTS FOR {}'.format(clf))\n",
    "    print(classification_report(test_target, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (iv) some ideas to help improve predictions... \n",
    "# - increase number of training examples.\n",
    "# - fewer ratings (ratings of 1-3 instead of 1-5)\n",
    "# - clean the testing and training data better (strip punctuation, ascii characters, etc) although \n",
    "#   I think the tfidf vectorizer is supposed to do that already"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
